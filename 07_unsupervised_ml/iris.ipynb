{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ZHAW-ZAV/TSO-FS25-students/blob/main/07_unsupervised_ml/iris.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "## PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features]), columns=features)\n",
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we calculate the covariance matrix\n",
    "cov_matrix = df_scaled.cov()\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we calculate the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors: {eigenvectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# first we need to standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features]), columns=features)\n",
    "df_scaled.describe()\n",
    "\n",
    "# then we calculate the covariance matrix\n",
    "cov_matrix = df_scaled.cov()\n",
    "cov_matrix\n",
    "\n",
    "# then we calculate the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "# print(f\"Eigenvalues: {eigenvalues}\")\n",
    "\n",
    "# Sort the eigenvalues and corresponding eigenvectors in descending order\n",
    "sorted_idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_idx]\n",
    "eigenvectors = eigenvectors[\n",
    "    :, sorted_idx\n",
    "]  # sort columns so each column is an eigenvector\n",
    "\n",
    "# Select the first 2 eigenvectors (columns) for projection\n",
    "eigenvectors_1 = eigenvectors[:, :1]\n",
    "\n",
    "# Project the data onto the 1 principal components\n",
    "df_pca = pd.DataFrame(np.dot(df_scaled, eigenvectors_1), columns=[\"PC1\"])\n",
    "px.scatter(df_pca, x=\"PC1\", y=[0] * len(df_pca), color=df[\"species\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D reduction\n",
    "\n",
    "**Task**: Obtain the 2 principal components and visualize the data in 2D with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D reduction\n",
    "\n",
    "**Task**: Obtain the 3 principal components and visualize the data in 3D with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "## K means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features]), columns=features)\n",
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose K centroids\n",
    "K = 3\n",
    "centroids = df_scaled.sample(n=K)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance between each point and each centroid\n",
    "distances_centroid1 = np.sqrt(np.sum((df_scaled - centroids.iloc[0]) ** 2, axis=1))\n",
    "distances_centroid2 = np.sqrt(np.sum((df_scaled - centroids.iloc[1]) ** 2, axis=1))\n",
    "distances_centroid3 = np.sqrt(np.sum((df_scaled - centroids.iloc[2]) ** 2, axis=1))\n",
    "# now assign cluster based on closest centroid\n",
    "clusters = np.argmin(\n",
    "    np.array([distances_centroid1, distances_centroid2, distances_centroid3]), axis=0\n",
    ")\n",
    "df_scaled[\"cluster\"] = clusters\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate the centroids\n",
    "centroids = df_scaled.groupby(\"cluster\").mean()\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat distance calculation and cluster assignment\n",
    "distances_centroid1 = np.sqrt(np.sum((df_scaled - centroids.iloc[0]) ** 2, axis=1))\n",
    "distances_centroid2 = np.sqrt(np.sum((df_scaled - centroids.iloc[1]) ** 2, axis=1))\n",
    "distances_centroid3 = np.sqrt(np.sum((df_scaled - centroids.iloc[2]) ** 2, axis=1))\n",
    "# now assign cluster based on closest centroid\n",
    "clusters = np.argmin(\n",
    "    np.array([distances_centroid1, distances_centroid2, distances_centroid3]), axis=0\n",
    ")\n",
    "df_scaled[\"cluster\"] = clusters\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat until centroids don't change\n",
    "change = True\n",
    "previous_centroids = centroids.copy()\n",
    "while change:\n",
    "    centroids = df_scaled.groupby(\"cluster\").mean()\n",
    "    # repeat distance calculation and cluster assignment\n",
    "    distances_centroid1 = np.sqrt(np.sum((df_scaled - centroids.iloc[0]) ** 2, axis=1))\n",
    "    distances_centroid2 = np.sqrt(np.sum((df_scaled - centroids.iloc[1]) ** 2, axis=1))\n",
    "    distances_centroid3 = np.sqrt(np.sum((df_scaled - centroids.iloc[2]) ** 2, axis=1))\n",
    "    #  assign cluster based on closest centroid\n",
    "    clusters = np.argmin(\n",
    "        np.array([distances_centroid1, distances_centroid2, distances_centroid3]),\n",
    "        axis=0,\n",
    "    )\n",
    "    df_scaled[\"cluster\"] = clusters\n",
    "    change = not np.array_equal(centroids, previous_centroids)\n",
    "    previous_centroids = centroids.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: add the obtained clusters to the 2D / 3D PCA and visualize the data in 2D / 3D with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# standardize the data\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features]), columns=features)\n",
    "\n",
    "# apply DBscan\n",
    "dbscan = DBSCAN(eps=0.6, min_samples=4)\n",
    "dbscan.fit(df_scaled)\n",
    "df_scaled[\"cluster\"] = dbscan.labels_\n",
    "# we apply PCA to plot in 2D\n",
    "df_dbscan = pd.DataFrame(np.dot(df_scaled[features], eigenvectors_2), columns=[\"PC1\", \"PC2\"])\n",
    "df_dbscan[\"cluster\"] = dbscan.labels_\n",
    "px.scatter(df_dbscan, x=\"PC1\", y=\"PC2\", color=\"cluster\", height=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
